\subsubsection{RQ1. Effectiveness on Try-Catch Necessity Checking}~\\
\indent {\em Baselines.} We compared {\xblock} with
GPT-3.5 in ChatGPT~\cite{ChatGPT}. Due to cost of using ChatGPT-3.5 on
OpenAI, we performed sampling on the Github dataset of 30,764
snippets. To obtain the confidence level of 95\%, we randomly selected
380 code snippets in which 190 are negative samples
(no \code{try-catch} block), and 190 are positive samples (at least
one \code{try-catch} block). We trained on Github dataset and
compared with GPT-3.5 on this sampled test set.

%the pre-trained CodeBERT without any fine-tuning steps. In the
%pre-trained CodeBERT, We add a randomly-initialized linear layer on
%top of the output vector of the \texttt{[CLS]} token, and use a
%softmax function to learn the decision.
We also compared {\xblock} with XRank~\cite{xrank-fse20} (XRank is
part of FuzzyCatch) on the Github dataset. XRank computed
the exception risk score for each API call. If a score of a call in
the snippet is higher than a threshold, we consider it as needing a
\code{try-catch} block.

{\em Procedure.} We randomly split both the positive and negative sets
in a dataset into 80\%, 10\%, and 10\% of the code
snippets for training, tuning, and testing. Meanwhile, we make sure
that each partition contains the equal amount of positive samples and
negative samples; and the training and tuning partitions do not
contain any duplicates. 

To request responses from ChatGPT, we construct prompt with the format
"question + code", where the question we pose is "Does the code below
need to catch any exceptions?\textbackslash n\textbackslash n".
Considering that the answers from ChatGPT for the same prompt may
vary, for each code snippet, we send the prompt three times through
the Chat Completions API.
%(https://platform.openai.com/docs/guides/gpt/chat-completions-api).
Labeling the responses has three steps. First, we check whether the
first word in each response is Yes or No. If it is a Yes, we assign a
positive label; If it is a No, we assign a negative label. Second, for
the responses that do not start with Yes or No, we read each response
and manually assign labels to them.  However, there are some cases
that we cannot, based on the response, easily determine whether or not
the code needs any try-catch blocks. The common reasons are (1) the
response is not informative enough in that it only relays some general
advice about exception handling, (2) In the text, it states that it is
uncertain about the case, or (3) the decision depends on background
knowledge about either the project structure or certain method
specifications. Thus, in evaluating the performance of ChatGPT,
for each metric, we calculate the lower and upper bound. In the upper
bound, all such uncertain cases are considered as correct,
%receive the ground truth labels, while
in the lower bound, all such cases are considered as incorrect.
%receive the opposite labels to their ground truths.
Lastly, we assign the final label for each instance from the majority
vote from three tries.

%We took all the code snippets from DeepEX and FuzzyCatch datasets. On
%the DeepEx dataset, we randomly split both the positive and negative
%data points into 80\%, 10\%, 10\%, in which 80\% of the code snippets
%as the training dataset, 10\% of the code snippets as the tuning
%dataset, and 10\% of the code snippets as the testing dataset for the
%baseline and {\tool}.

%THIS PART FOR RQ5
%----------------
%And on the FuzzyCatch dataset, we directly use
%the trained model from the DeepEx dataset and test it on the
%FuzzyCatch dataset for both the baseline and {\tool}.

{\em Tuning.} We trained {\tool} for $15$ epochs with the following key hyper-parameters: (1) Batch size is set to $32$; (2) Learning rate is set to $0.000006$; (3) Weight decay is set to $0.01$. We select the model with the lowest overall loss. 

%We tuned {\tool} with autoML~\cite{NNI} for the
%following key hyper-parameters to have the best performance: (1) Epoch
%size (50, 100, 150); (2) Batch size (32, 64, 128); (3) Learning rate
%(0.001, 0.003, 0.005); (4) Vector length of feature embeddings and its
%output (64, 128, 256); (5) Number of R-GCN layers (4, 6, 8).

{\em Metrics.} We use \textbf{Recall, Precision}, and {\bf F-score} to
evaluate the performance of the approaches. They are calculated as
$Recall = \frac{TP}{TP+FN}$, $Precision = \frac{TP}{TP+FP}$, $F$-score
$=$ $\frac{2*Recall*Precision}{Recall+Precision}$. $TP$: true
positive, $FN$: false negative, and $FP$: false positive.
