\subsubsection{Effectiveness on Try-Catch Necessity Checking (RQ1)}~\\
\indent {\em Baselines.} We compared {\xblock} with
GPT-3.5~\cite{ChatGPT}. Due to cost of using GPT-3.5 on OpenAI, we
performed sampling on the GitHub dataset of 30,764 snippets. To obtain
the confidence level of 95\%, we randomly selected 380 code snippets
in which 190 are negative samples (no \code{try-catch} block), and 190
are positive samples (at least one \code{try-catch} block). We trained
on GitHub dataset and compared with GPT-3.5 on this sampled test set.

%the pre-trained CodeBERT without any fine-tuning steps. In the
%pre-trained CodeBERT, We add a randomly-initialized linear layer on
%top of the output vector of the \texttt{[CLS]} token, and use a
%softmax function to learn the decision.
We also compared {\xblock} with XRank~\cite{xrank-fse20} (XRank is
part of FuzzyCatch) on the GitHub dataset. XRank computed
the exception risk score for each API call. If a score of a call in
the snippet is higher than a threshold, we consider it as needing a
\code{try-catch} block.

{\em Procedure.} We randomly split both the positive and negative sets
in a dataset into 80\%, 10\%, and 10\% of the code
snippets for training, tuning, and testing. Meanwhile, we make sure
that each partition contains the equal amount of positive samples and
negative samples; and the training and tuning partitions do not
contain any duplicates.

To request responses from ChatGPT, we construct prompt with the format
"question + code", where the question we pose is "Does the code below
need to catch any exceptions?\textbackslash n\textbackslash n".
Considering that the answers from ChatGPT for the same prompt may
vary, for each code snippet, we send the prompt three times through
the Chat Completions API.
%(https://platform.openai.com/docs/guides/gpt/chat-completions-api).
Labeling the responses has three steps. First, we check whether the
first word in each response is Yes or No. If it is a Yes, we assign a
positive label; If it is a No, we assign a negative label. Second, for
the responses that do not start with Yes or No, we read each response
and manually assign labels to them.  However, there are some cases in
which it is hard to make the decision on whether or not the code needs
any try-catch blocks. The common scenarios are (1) the response is not
informative enough, as it only relays a general advice about exception
handling, (2) the response states that it is uncertain whether a
try-catch block is needed, or (3) the decision making depends on the
background knowledge on either the project structure or certain method
specifications. Thus, in evaluating GPT-3.5's performance, for
these responses, given the benefit of doubts, we assign correct
prediction labels to them, assuming the best performance of GPT-3.5.
%for each metric, we calculate the lower and upper bound. In the upper
%bound, all such uncertain cases are considered as correct,
%in the lower bound, all such cases are considered as incorrect.
We assign the final label for each instance from the majority
vote of the three attempts.

%We took all the code snippets from DeepEX and FuzzyCatch datasets. On
%the DeepEx dataset, we randomly split both the positive and negative
%data points into 80\%, 10\%, 10\%, in which 80\% of the code snippets
%as the training dataset, 10\% of the code snippets as the tuning
%dataset, and 10\% of the code snippets as the testing dataset for the
%baseline and {\tool}.

%THIS PART FOR RQ5
%----------------
%And on the FuzzyCatch dataset, we directly use
%the trained model from the DeepEx dataset and test it on the
%FuzzyCatch dataset for both the baseline and {\tool}.

{\em Tuning.} We trained {\tool} for $15$ epochs with the following key hyper-parameters: (1) Batch size is set to $32$; (2) Learning rate is set to $0.000006$; (3) Weight decay is set to $0.01$. We select the model with the lowest overall validation loss.

%We tuned {\tool} with autoML~\cite{NNI} for the
%following key hyper-parameters to have the best performance: (1) Epoch
%size (50, 100, 150); (2) Batch size (32, 64, 128); (3) Learning rate
%(0.001, 0.003, 0.005); (4) Vector length of feature embeddings and its
%output (64, 128, 256); (5) Number of R-GCN layers (4, 6, 8).

{\em Metrics.} We use \textbf{Precision, Recall}, and {\bf F1-score} to
evaluate the performance of the approaches. They are calculated as follows.
$Precision = \frac{TP}{TP+FP}$, $Recall = \frac{TP}{TP+FN}$, $F1$-score
$=$ $\frac{2*Recall*Precision}{Recall+Precision}$. $TP$: true
positive, $FN$: false negative, and $FP$: false positive.
