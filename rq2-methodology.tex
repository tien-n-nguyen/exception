\subsubsection{RQ2. Effectiveness on Try-Catch Statement Detection\\}

\indent {\em Baselines.} We compared {\xstate} against the pre-trained
CodeBERT model without any fine-tuning step as in RQ1.

{\em Procedure.} We used the same procedure as in RQ1.

{\em Tuning.} We used the same tuning as in RQ1.

{\em Metrics.} We used the same metrics as in RQ1. However, we
computed Recall, Precision, and F-score in two ways. First, we
evaluated {\xstate} in connection with {\xblock}. That is, we consider
a correct case if {\xblock} gives a correct prediction on whether
\code{try-catch} blocks are needed, {\em and} {\xstate} produces correct tags for all the statements when there exists a \code{try-catch} block. Second,
we evaluated {\xstate} as individual. That is, we assumed that
{\xblock} predicts correctly whether a code snippet needs
\code{try-catch} blocks or not. Thus, we put the code snippets that
need such blocks in the oracle and used {\xstate} to predict the groupings of
statements for the blocks in those snippets.

%For a snippet that needs \code{try-catch}, but was predicted as
%not, we consider it as incorrect because the resulting statements
%predicted from {\xstate} do not make sense for incorrect {\xblock}'s
%detection. A snippet that does not need \code{try-catch}, but were
%predicted as yes, we also consider them as incorrect for the same
%reason. For a snippet that does not need a \code{try-catch} block,
%and was predicted as not needing, we consider it as correct.
%For a snippet needing such a block, and was predicted as yes,
%we consider if the current statement was predicted correctly or not.

%Thus, for the evaluation of {\xstate}, we used the set of code
%snippets that {\xblock} predicted correctly as needing a
%\code{try-catch} block. We used the same data splitting scheme with
%80\%, 10\%, and 10\% for training, tuning, and testing as in RQ1. Each
%code snippet in the testing set and the trained R-GCN model in
%\xblock} are the input of the GNNExplainer model in {\xstate} in this
%experiment.

%{\em Tuning.} We used the same parameters as in
%GNNExplainer~\cite{GNNExplainer}. It also has a parameter on the limit
%of the number $N$ of the nodes in the explanation sub-graph. We
%varied $N$ from 1 to 10. The average number of statements in a
%\code{try-catch} block in Github dataset is 5.9.

%{\em Metrics.} If {\xstate} predicts for a statement correctly if it
%is in a \code{try-catch} block or not, we count it as a correct case.
%Otherwise, it is an incorrect one. \textbf{Accuracy} is
%defined as the ratio between the number of correct statements over the
%total number of statements.
