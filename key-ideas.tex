\subsection{Key Ideas}
\label{key:sec}

We introduce {\tool} with three functions for exception handling
recommendation: given an (in)complete Java code snippet, it will

\vspace{-3pt}
\begin{enumerate}

\item predict if a \code{try-catch} block is required ({\xblock}),

\item point out which statements in the code snippet need to be placed within
a \code{try} block ({\xstate}), and

\item suggest the exception types to be in the \code{catch}
clause ({\xtype}).

\end{enumerate}

From Observations, we design {\tool} with the following
ideas:

\subsubsection{[Key Idea 1] Neural Network-based Approach to Exception Handling Recommendation}
Instead of deterministically deriving the exceptions to be handled for
a given (incomplete) code snippet, from Observation~\ref{ob2}, we
use a machine learning-based approach to suggest to properly handle the
exceptions via the above tasks.
%to learn to analyze that snippet to recommend if a \code{try-catch}
%block is needed and the statements to be placed within the
%\code{try-catch} block, and to recommend the exception types in the
%\code{catch} clause.
By learning from the \code{try-catch} blocks of complete code in
the open-source projects in training, {\tool} can
suggest exception handling.

%: 1) deciding what
%statements in the given code snippet to be placed in a
%\code{try-catch} block, and 2) deciding what exceptions to be handled.



%\vspace{2pt}
\subsubsection{[Key Idea 2] Leveraging a Code-aware Language Model to learn Context with Dependencies among API Elements and Relations with Exceptions.}
  
Instead of computing the association scores for the pairs between API
elements and exception types as in IR-based approaches, we leverage as
the context the complete code in the training corpus, to fine-tune a
code-aware language model (LM)~to learn the dependencies among the API
elements, and their relations with the corresponding exception types.
%
In predicting for a snippet $C$, {\tool} uses the LM to take the
context with the dependencies among the API elements in $C$ and suggest
exception handling in three tasks (Observation~\ref{ob4}). The
relations between API elements and the exceptions also help the model
suggest the exception types.

%context and dependencies among the API elements to learn their
%identities implicitly (see Observation~\ref{ob4}). Importantly, that
%leads to the learning of the relations between the key API elements in
%the context and the handled exception types (Observation~\ref{ob3}).

%==========================
%\vspace{2pt}
%\subsubsection{{\bf [Key Idea 2] Leveraging Language Model to learn Context to avoid name ambiguity and Learning the Relations between API elements and Exception types}}
%Instead of learning only the associations between an API element and
%exception types as in IR-based approaches, we leverage as the context
%the complete code in the training corpus, which are parsable and
%provide the identities (i.e., FQNs) of the API elements. In predicting
%for a code snippet, {\tool} will also leverage the context and
%dependencies among the API elements to learn their identities
%implicitly (see Observation~\ref{ob4}). Importantly, that leads to the
%learning of the relations between the key API elements in the context
%and the handled exception types (Observation~\ref{ob3}).
%==========================

%\subsubsection{{\bf [Key Idea 3] Dual-Task Learning between {\xstate} and {\xtype}}} The learning of deciding what statements to be in a \code{try-catch}
%block could benefit much on the learning of deciding what exception
%types to be handled. In Figure~\ref{fig:example4}, if a model learns
%that line 3 (with \code{new\-Buffered\-Reader}) and line 5 (with
%\code{read\-Line}) need to be in \code{try-catch}, it could learn from
%the code corpus and decide \code{IOException} as the exception type to
%be handled. In contrast, if a model learns correctly that the
%\code{IOException} needs to be handled, from the history, it can learn
%that \code{new\-Buffered\-Reader} and \code{read\-Line} can throw that
%exception and determine that the line 3 and 5 need to be in
%\code{try-catch}.

%{\em
%\subsubsection{{\bf [Key Idea 3] Span-based ...}}
%We seek inspiration from the neural network-based span-based ...
%approaches~\cite{?} in Natural Language Processing (NLP). They
%successfully learn ... Following suit, we design \tool to learn the
%representations for the statements in source code so as to learn the
%span ...}

\subsubsection{[Key Idea 3] Leveraging Sequence Chunking with a Language Model and Multi-tasking} 
Inspired by the sequence chunking
techniques~\cite{sequence-chunking-aaai17} in NLP, we formulate our
problem as identifying one or multiple chunks of consecutive
statements that need to be placed within \code{try} blocks. We
leverage and fine-tune the language model
CodeBERT~\cite{codebert-emnlp20} to learn the relations among the
statements with the API elements. We also utilize a multi-tasking
framework for three tasks: {\xblock}, {\xstate}, and {\xtype}, as the
knowledge learned from one task can enhance the performance of the others.


%We also leverage the multi-tasking
%framework for all three tasks {\xblock}, {\xstate}, and {\xtype}
%because the learning for one task can benefit for another.

%task and vice versa.

%\subsubsection{{\bf [Key Idea 3] Leveraging Explainable AI to predict statements in a try-catch block}} To predict which statements in a given code snippet that need to be placed in a \code{try-catch} block, we leverage the explainable ML
%model, GNNExplainer~\cite{GNNExplainer}, that {\em ``explains'' why
%  the R-GCN model has arrived at its decision}. Specifically, {\tool}
%takes as input the given code, the trained GCN model along with its
%decision on the classification of the task {\xblock} on the given
%code. GNNExplainer will produce the subgraph of the PDG of the given
%code, which is called the {\em explanation subgraph}, that minimizes
%the prediction scores between using the original PDG and using the
%explanation subgraph. That is, that minimal explanation subgraph
%contains the crucial statements that are most decisive to the
%classification result for {\xblock} on the need of a \code{try-catch}
%block.  Thus, those statements returned from GNNExplainer can be
%considered as the statements that need to placed in that block.
