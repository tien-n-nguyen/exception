\subsection{Empirical Methodology}

\subsubsection{Datasets}

%jodatime, jdk, Android, xstream, gwt, hibernate

We conducted experiments on two datasets: 1) {\em Github dataset}
for intrinsic evaluation on exception handling recommendation tasks
({\xblock}, {\xstate}, {\xtype}), and 2) {\em FuzzyCatch}
dataset~\cite{xrank-fse20} for extrinsic evaluation on
exception-related bug detection.
%
%FuzzyCatch dataset was provided by the authors of
%XRank/XHand~\cite{xrank-fse20}. We used FuzzyCatch dataset for our
%extrinsic evaluation.
%
We collected the Github dataset as follows. We first chose in Github
5,726 Java projects with the highest ratings that use
the following libraries: jodatime, JDK, Android, xtream, GWT, and
Hibernate. These are the well-established libraries that have been used
in several prior research on the topics related to
APIs~\cite{icse18,liveapi14}. We then selected the methods with at
least one \code{try-catch} block, which was not part of any fix in
a later version.
%If there is one \code{try-catch}
%block, we take the body of the method as the code
%snippet. If there are multiple \code{try-catch} blocks in a method, we
%split the method into multiple code snippets by checking the above and
%below statements, one by one, starting from the closest one for each
%\code{try-catch} block, to verify if the statement is included in the
%other block. If not, we put it in the current code snippet. If yes, we
%finish the current one. 
In total, we have 153,823 code snippets containing \code{try-catch}
blocks as positive samples. We also randomly selected from the same
Github projects the same amount of code snippets that do not have any
\code{try-catch} block as the negative samples.

For extrinsic evaluation, we used FuzzyCatch dataset, provided by the
authors of XRank/XHand~\cite{xrank-fse20}, which contains 750 samples
of methods with exception-related bugs (missing \code{try-catch}
blocks or missing catching some exceptions). We also randomly
selected from the projects in FuzzyCatch dataset the same amount of
code snippets with no exception-related bugs as the negative samples.

%We have conducted our experiments to evaluate {\tool} on the dataset DeepEx that we collected, and the existing dataset FuzzyCatch provided in a prior work~\cite{nguyen2020code}.

%To build the dataset DeepEx, we first collect XXX methods from XXX Java projects. And then, we picked XXX methods that contain at least one try-catch block from all methods. To avoid the influence of multiple try-catch blocks, we split the methods into code snippets by checking the above and below statements, one by one, starting from the closest one for each try-catch block, to verify if the statement is included in the other try-catch block. If not, we put it in the current code snippet. If yes, we stop here and finish the code snippet.

%Following the steps mentioned above, we have XXX code snippets containing one try-catch block as positive data. Then, we randomly create the same amount of code snippets that do not contain a try-catch block as negative data from the Java projects we collected. So, in total, the DeepEx dataset includes XXX code snippets. As for the FuzzyCatch dataset, it includes 1,000 data, and all of them are positive data.


\input{rq1-methodology}

\input{rq2-methodology}

\input{rq3-methodology}

%\subsubsection{RQ2. Effectiveness on Try-Catch Statement Detection\\}

%\indent {\em Baselines.} None. FuzzyCatch (XRank/XHand) does not have this.

%{\em Procedure.} We processed the Github dataset for this experiment
%as follows. For the snippets that need \code{try-catch}, but were
%predicted as not, we consider them as incorrect because the resulting
%statements predicted from {\xstate} do not make sense for incorrect
%{\xblock}'s detection. The snippets that do not need \code{try-catch},
%but were predicted as yes, we also consider them as incorrect for the
%same reason. Thus, for the evaluation of {\xstate}, we used the set of
%code snippets that {\xblock} predicted correctly as needing a
%\code{try-catch} block. We used the same data splitting scheme with
%80\%, 10\%, and 10\% for training, tuning, and testing as in RQ1. Each code
%snippet in the testing set and the trained R-GCN model in {\xblock}
%are the input of the GNNExplainer model in {\xstate} in this
%experiment.

%{\em Tuning.} We used the same parameters as in
%GNNExplainer~\cite{GNNExplainer}. It also has a parameter on the limit
%of the number $N$ of the nodes in the explanation sub-graph. We
%varied $N$ from 1 to 10. The average number of statements in a
%\code{try-catch} block in Github dataset is 5.9.

%{\em Metrics.} If {\xstate} predicts for a statement correctly if it
%is in a \code{try-catch} block or not, we count it as a correct case.
%Otherwise, it is an incorrect one. \textbf{Accuracy} is
%defined as the ratio between the number of correct statements over the
%total number of statements.



\subsubsection{RQ4. }

%
%In this experiment, we aim to evaluate the impact on the performance
%of the key features: sequences of code tokens and Abstract Syntax Tree.
%We removed one key feature at a time and compared the performance with
%the original model to evaluate its impact. We used the same evaluation
%metrics as in the previous experiments. Because our model is designed
%with R-GCN, we cannot remove it to evaluate the impact of dependencies.

%{\em Metrics.} We use the same evaluation metrics as RQ1 and RQ3 to evaluate the impact of the different features on the experiment results.





\subsubsection{RQ5. Extrinsic Evaluation on Exception-related Bug Detection}~\\
\indent {\em Baselines.} FuzzyCatch~\cite{xrank-fse20} leverages XRank
to detect the exception-related bugs, which are the code snippets that
were supposed to handle exceptions, but missed
\code{try-catch} blocks and/or exceptions.
%We used {\tool} to detect such bugs and compare with FuzzyCatch.

{\em Procedure.} We trained {\tool} on the Github dataset and detected
the exception-related bugs in FuzzyCatch bug dataset via {\xblock}.

%To make the FuzzyCatch dataset a balanced dataset for a fair comparison, we randomly select the same number of non-buggy code snippets comparing the bugs as the negative samples from the methods in the FuzzyCatch dataset.

{\em Metrics.} We compared the result against the oracle in FuzzyCatch
dataset. If {\xblock} correctly detects a buggy snippet (missing a
\code{try-catch} block or exceptions), we consider it as correct.
Otherwise, it is a miss. We use {\bf Recall}, {\bf Precision}, and
{\bf F-score} as in RQ1.

\subsubsection{RQ6. Ablation Study}~\\
We aim to evaluate the contribution of fine-tuning in {\tool}. We
compared {\tool} against CodeBERT without fine-tuning.
