\subsection{Empirical Methodology}

\subsubsection{Datasets}

%jodatime, jdk, Android, xstream, gwt, hibernate

We conducted experiments on two datasets: 1) {\em Github dataset}
for intrinsic evaluation on exception handling recommendation tasks
({\xblock}, {\xstate}, {\xtype}), and 2) {\em FuzzyCatch}
dataset~\cite{xrank-fse20} for extrinsic evalution on
exception-related bug detection.
%
%FuzzyCatch dataset was provided by the authors of
%XRank/XHand~\cite{xrank-fse20}. We used FuzzyCatch dataset for our
%extrinsic evaluation.
%
We collected the Github dataset as follows. We first chose in Github
5,726 Java projects with the highest ratings that use
the following libraries: jodatime, JDK, Android, xtream, GWT, and
Hibernate. These are the well-estabished libraries that have been used
in several prior research on the topics related to
APIs~\cite{icse18,liveapi14}. We then selected the methods with at
least one \code{try-catch} block. If there is one \code{try-catch}
block, we take the body of the method as the code
snippet. If there are multiple \code{try-catch} blocks in a method, we
split the method into multiple code snippets by checking the above and
below statements, one by one, starting from the closest one for each
\code{try-catch} block, to verify if the statement is included in the
other block. If not, we put it in the current code snippet. If yes, we
finish the current one. In total, we have
19,379 code snippets containing one
\code{try-catch} block as positive samples. We also randomly selected
from the same Github projects the same amount of code snippets that do
not have any \code{try-catch} block as the negative samples.

For extrinsic evaluation, we used FuzzyCatch dataset, provided by the
authors of XRank/XHand~\cite{xrank-fse20}, which contains 608 samples
of methods with exception-related bugs (missing \code{try-catch}
blocks or missing catching some exceptions).  We also randomly
selected from the projects in FuzzyCatch dataset the same amount of
code snippets with no exception-related bugs as the negative samples.

%We have conducted our experiments to evaluate {\tool} on the dataset DeepEx that we collected, and the existing dataset FuzzyCatch provided in a prior work~\cite{nguyen2020code}.

%To build the dataset DeepEx, we first collect XXX methods from XXX Java projects. And then, we picked XXX methods that contain at least one try-catch block from all methods. To avoid the influence of multiple try-catch blocks, we split the methods into code snippets by checking the above and below statements, one by one, starting from the closest one for each try-catch block, to verify if the statement is included in the other try-catch block. If not, we put it in the current code snippet. If yes, we stop here and finish the code snippet.

%Following the steps mentioned above, we have XXX code snippets containing one try-catch block as positive data. Then, we randomly create the same amount of code snippets that do not contain a try-catch block as negative data from the Java projects we collected. So, in total, the DeepEx dataset includes XXX code snippets. As for the FuzzyCatch dataset, it includes 1,000 data, and all of them are positive data.


\subsubsection{RQ1. Effectiveness on Try-Catch Necessity Checking\\}

\indent {\em Baselines.} We compared {\xblock} against
XRank~\cite{xrank-fse20} (XRank is part of FuzzyCatch tool). XRank
computed the exception risk score for each API call.
%(i.e., the potential need of \code{try-catch} block).
If one score of a call in the snippet is higher than a threshold, 
it is considered as needing a \code{try-catch} block.

{\em Procedure.} We used the Github dataset and randomly split both
the positive and negative sets into 80\%, 10\%, and 10\% of the code
snippets for training, tuning, and testing.

%We took all the code snippets from DeepEX and FuzzyCatch datasets. On
%the DeepEx dataset, we randomly split both the positive and negative
%data points into 80\%, 10\%, 10\%, in which 80\% of the code snippets
%as the training dataset, 10\% of the code snippets as the tuning
%dataset, and 10\% of the code snippets as the testing dataset for the
%baseline and {\tool}.

%THIS PART FOR RQ5
%----------------
%And on the FuzzyCatch dataset, we directly use
%the trained model from the DeepEx dataset and test it on the
%FuzzyCatch dataset for both the baseline and {\tool}.

{\em Tuning.} We tuned {\tool} with autoML~\cite{NNI} for the
following key hyper-parameters to have the best performance: (1) Epoch
size (50, 100, 150); (2) Batch size (32, 64, 128); (3) Learning rate
(0.001, 0.003, 0.005); (4) Vector length of feature embeddings and its
output (64, 128, 256); (5) Number of R-GCN layers (4, 6, 8).

{\em Metrics.} We use \textbf{Recall, Precision}, and {\bf F-score} to
evaluate the performance of the approaches. They are calculated as
$Recall = \frac{TP}{TP+FN}$, $Precision = \frac{TP}{TP+FP}$, $F$-score
$=$ $\frac{2*Recall*Precision}{Recall+Precision}$. $TP$: true
positive, $FN$: false negative, and $FP$: false positive.

\subsubsection{RQ2. Effectiveness on Try-Catch Statement Detection\\}

\indent {\em Baselines.} None. FuzzyCatch (XRank/XHand) does not have
this.

{\em Procedure.} We processed the Github dataset for this experiment
as follows. For the snippets that need \code{try-catch}, but were
predicted as not, we consider them as incorrect because the resulting
statements predicted from {\xstate} do not make sense for incorrect
{\xblock}'s detection. The snippets that do not need \code{try-catch},
but were predicted as yes, we also consider them as incorrect for the
same reason. Thus, for the evaluation of {\xstate}, we used the set of
code snippets that {\xblock} predicted correctly as needing a
\code{try-catch} block. We used the same data splitting scheme with
80\%, 10\%, and 10\% for training, tuning, and testing as in RQ1. Each code
snippet in the testing set and the trained R-GCN model in {\xblock}
are the input of the GNNExplainer model in {\xstate} in this
experiment.

%In this RQ, \tool is evaluated on the DeepEx dataset. Because only the data points that are predicted as needing a try-catch block, the \tool will run the try-catch statement detection on them. To fully use the dataset and evaluate \tool on more data, here we estimate the accuracy of RQ1 prediction is 100\% which means that we use all positive data in DeepEx dataset as the data we use in this RQ. We do the same data split as RQ1 to train, validate and test the model performance.

{\em Tuning.} We used the same parameters as in
GNNExplainer~\cite{GNNExplainer}. It also has a parameter on the limit
of the number $N$ of the nodes in the explanation sub-graph. We
varied $N$ from 1 to 10. The average number of statements in a
\code{try-catch} block in Github dataset is 5.9.

%We tuned {\tool} in this RQ by testing different node number limits for generated sub-graph. The range of the node number limit we tested is from $1$ to $10$ nodes.

{\em Metrics.} If {\xstate} predicts for a statement correctly if it
is in a \code{try-catch} block or not, we count it as a correct case.
Otherwise, it is an incorrect one. \textbf{Accuracy} is
defined as the ratio between the number of correct statements over the
total number of statements.

%For each statement in the try-catch block, if
%\tool included it in the generated sub-graph, we think the prediction
%on it is correct. Otherwise, it is incorrect. The Accuracy is
%calculated as $Accuracy = \frac{Correct}{Correct + Incorrect}$.

\subsubsection{RQ3. Effectiveness on Exception Type Recommendation\\}

{\em Baselines.} None. XRank/Xhand do not support this feature.

%We compared {\tool} against the state-of-the-art exception type recommendation approach XRank~\cite{nguyen2020code}.

{\em Procedure.} We processed the Github dataset for this experiment
in the same manner as in RQ2 because the result of {\xtype} (i.e.,
what exception types need to be in the \code{catch} clause) makes
sense only when one knows that the given code snippet needs to have a
\code{try-catch} block. Thus, for the evaluation of {\xtype}, we used
the set of code snippets that {\xblock} predicted correctly as needing
a try-catch block. We used the same splitting as RQ2. The exception
types in the \code{catch} clauses are used as the labels.

%In this RQ, \tool is evaluated on the DeepEx dataset. Because only the data points that \tool can successfully predict all statements in the try-catch block successfully, the \tool will run the exception type prediction on them. Similarly, as RQ2, to fully use the dataset and evaluate \tool on more data, here we estimate the Accuracy of RQ2 prediction is 100\% which means that we use all positive data in DeepEx dataset as the data we use in this RQ. We do the same data split as RQ1 and RQ2 to train, validate and test the model performance.

{\em Tuning.} We tuned {\tool} with autoML~\cite{NNI} for the
following key hyper-parameters to have the best performance: (1) Epoch
size (50, 100, 150); (2) Batch size (32, 64, 128); (3) Learning rate
(0.001, 0.003, 0.005); (4) Vector length of feature embeddings and its
output (64, 128, 256); (5) Number of R-GCN layers (4, 6, 8).

{\em Metrics.} Let us use $E$ and $P$ to denote the set of the
exception types in the oracle and the set of predicted
ones for one code snippet in the testing set. We aim to measure 1) how
precise {\tool} predicts the exception types ({\bf Precision}), i.e.,
how accurate the predicted types in $P$, and 2) how much {\tool} can
cover in its prediction with respect to the oracle ({\bf Recall}),
i.e., how much of $E$ that the predicted set $P$ can cover. Toward
measuring Precision and Recall, we define {\bf Hit-$n$} as the number
of the cases in which the predicted set $P$ contains
{\em at least} $n$ correct exception types, i.e., $P$ and $E$ overlaps
{\em at least} $n$ exception types regardless of the sizes of both
sets.

In Github dataset, more than 98.1\% of the code snippets have 1--3
exception types in a \code{catch} clause. Thus, we compute Hit-$n$,
Precision, and Recall for the size of $E$ (|$E$|) from 1--3 and 3+,
and for the size of $P$ (|$P$|) from 1--3 and 3+.
%
When computing Recall, we use the set $E$ as the basis. Recall at a
size $N_E$ of $E$ is computed as the ratio between Hit-$n$ at that
size and the total number of cases with that size $N_E$. We compute
Recall for all $N_E$ = 1--3, and 3+, and $n$=1--$N_E$. We also define
{\bf \code{Hit-All}$_{Rec}$} as \code{Hit}-$n$ when the number $n$ of
overlapping exception types is equal to |$E$|, i.e., all exception
types in the oracle set for a code snippet are covered.
%
When computing Precision, we use the set $P$ as the basis. Precision
at a size $N_P$ of $P$ is computed as the ratio between Hit-$n$ at
that size and the total number of cases with that size $N_P$. We
compute Precision for all $N_P$ = 1--3, and $n$=1--$N_P$. We also
define {\bf \code{Hit-All}$_{Prec}$} as \code{Hit}-$n$ when the number $n$
of overlapping exception types is equal to |$P$|, i.e., all predicted
types in the predicted set $P$ for a snippet are
correct.


%In this RQ, we use \textbf{Hit-n} as the evaluation metrics. Hit-n
%here means within the true labeled statement set, there are at least
%\textbf{n} statements predicted correctly by \tool.

\subsubsection{RQ4. Ablation Study}

In this experiment, we aim to evaluate the impact on the performance
of the key features: sequences of code tokens and Abstract Syntax Tree.
We removed one key feature at a time and compared the performance with
the original model to evaluate its impact. We used the same evaluation
metrics as in the previous experiments. Because our model is designed
with R-GCN, we cannot remove it to evaluate the impact of dependencies.

%{\em Metrics.} We use the same evaluation metrics as RQ1 and RQ3 to evaluate the impact of the different features on the experiment results.

\subsubsection{RQ5. Extrinsic Evaluation on Exception-related Bug Detection\\}


\indent {\em Baselines.} FuzzyCatch~\cite{xrank-fse20} leverages XRank
to detect the exception-related bugs, which are the code snippets that
were supposed to handle exceptions, but missed
\code{try-catch} blocks and/or exceptions.
%We used {\tool} to detect such bugs and compare with FuzzyCatch.

{\em Procedure.} We trained {\tool} on the Github dataset and detected
the exception-related bugs in FuzzyCatch bug dataset.

%To make the FuzzyCatch dataset a balanced dataset for a fair comparison, we randomly select the same number of non-buggy code snippets comparing the bugs as the negative samples from the methods in the FuzzyCatch dataset.

{\em Metrics.} We compared the result against the oracle in FuzzyCatch
dataset. If a model correctly detects a (non)buggy snippet (missing a
\code{try-catch} block or exceptions), we consider it as correct.
Otherwise, it is a miss. We use {\bf Recall}, {\bf Precision}, and
{\bf F-score} as in RQ1.
