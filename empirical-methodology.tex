\vspace{-6pt}
\subsection{Empirical Methodology}

\subsubsection{Datasets}

%jodatime, jdk, Android, xstream, gwt, hibernate

We conducted experiments on two datasets: 1) {\em GitHub dataset}
for intrinsic evaluation on exception handling recommendation tasks
({\xblock}, {\xstate}, {\xtype}), and 2) {\em FuzzyCatch}
dataset~\cite{xrank-fse20} for extrinsic evaluation on
exception-related bug detection.
%
%FuzzyCatch dataset was provided by the authors of
%XRank/XHand~\cite{xrank-fse20}. We used FuzzyCatch dataset for our
%extrinsic evaluation.
%
We collected the GitHub dataset as follows. We first chose in GitHub
5,726 Java projects with the highest ratings that use
JDK and Android libraries.
%: jodatime, JDK, Android, xtream, GWT, and Hibernate.
These are the well-established libraries that have been used in
several prior research on the topics related to
APIs~\cite{icse18,liveapi14}.
%
We then selected the methods with at least one \code{try-catch} block
as positive samples, and we also randomly selected from the same
GitHub projects the same amount of code snippets that do not have any
\code{try-catch} block as the negative samples. In total, we have 246,118
code snippets for training, 30,764 for validation, and 30,764 for
testing. In 30,764 testing samples, there are an equal number of
positive and negative ones (15,382).

%There are the equal number of positive/negative samples in 30,764
%testing ones.

%We then selected the methods with at least one \code{try-catch} block,
%which was not part of any fix in a later version. In total, we have
%246,118 code snippets for training, and 30,764 for validation.  We
%also randomly selected from the same GitHub projects the same amount
%of code snippets that do not have any \code{try-catch} block as the
%negative samples. In total, we have 30,764 instances for testing.


For extrinsic evaluation, we used FuzzyCatch dataset, provided by the
authors of XRank/XHand~\cite{xrank-fse20}, which contains 609 Android
incomplete code snippets with exception-related bugs (missing
\code{try-catch} blocks or exception types). Finally, we used 553 code snippets
because the others are not valid for the experiment.

%Tien
%We also randomly selected from the projects in FuzzyCatch dataset the
%same amount of snippets with no exception-related bugs as the negative
%samples.

%We have conducted our experiments to evaluate {\tool} on the dataset DeepEx that we collected, and the existing dataset FuzzyCatch provided in a prior work~\cite{nguyen2020code}.

%To build the dataset DeepEx, we first collect XXX methods from XXX Java projects. And then, we picked XXX methods that contain at least one try-catch block from all methods. To avoid the influence of multiple try-catch blocks, we split the methods into code snippets by checking the above and below statements, one by one, starting from the closest one for each try-catch block, to verify if the statement is included in the other try-catch block. If not, we put it in the current code snippet. If yes, we stop here and finish the code snippet.

%Following the steps mentioned above, we have XXX code snippets containing one try-catch block as positive data. Then, we randomly create the same amount of code snippets that do not contain a try-catch block as negative data from the Java projects we collected. So, in total, the DeepEx dataset includes XXX code snippets. As for the FuzzyCatch dataset, it includes 1,000 data, and all of them are positive data.


\input{rq1-methodology}

\input{rq2-methodology}

\input{rq3-methodology}

%\subsubsection{RQ2. Effectiveness on Try-Catch Statement Detection\\}

%\indent {\em Baselines.} None. FuzzyCatch (XRank/XHand) does not have this.

%{\em Procedure.} We processed the GitHub dataset for this experiment
%as follows. For the snippets that need \code{try-catch}, but were
%predicted as not, we consider them as incorrect because the resulting
%statements predicted from {\xstate} do not make sense for incorrect
%{\xblock}'s detection. The snippets that do not need \code{try-catch},
%but were predicted as yes, we also consider them as incorrect for the
%same reason. Thus, for the evaluation of {\xstate}, we used the set of
%code snippets that {\xblock} predicted correctly as needing a
%\code{try-catch} block. We used the same data splitting scheme with
%80\%, 10\%, and 10\% for training, tuning, and testing as in RQ1. Each code
%snippet in the testing set and the trained R-GCN model in {\xblock}
%are the input of the GNNExplainer model in {\xstate} in this
%experiment.

%{\em Tuning.} We used the same parameters as in
%GNNExplainer~\cite{GNNExplainer}. It also has a parameter on the limit
%of the number $N$ of the nodes in the explanation sub-graph. We
%varied $N$ from 1 to 10. The average number of statements in a
%\code{try-catch} block in GitHub dataset is 5.9.

%{\em Metrics.} If {\xstate} predicts for a statement correctly if it
%is in a \code{try-catch} block or not, we count it as a correct case.
%Otherwise, it is an incorrect one. \textbf{Accuracy} is
%defined as the ratio between the number of correct statements over the
%total number of statements.



\input{rq4-methodology}
%
%In this experiment, we aim to evaluate the impact on the performance
%of the key features: sequences of code tokens and Abstract Syntax Tree.
%We removed one key feature at a time and compared the performance with
%the original model to evaluate its impact. We used the same evaluation
%metrics as in the previous experiments. Because our model is designed
%with R-GCN, we cannot remove it to evaluate the impact of dependencies.
%{\em Metrics.} We use the same evaluation metrics as RQ1 and RQ3 to evaluate the impact of the different features on the experiment results.

\vspace{2pt}
\subsubsection{Extrinsic Evaluation on Exception Bug Detection (RQ5)}~\\
\noindent {\em Baselines.} We compared with
FuzzyCatch~\cite{xrank-fse20} in exception-related bug detection,
i.e., missing \code{try-catch} blocks and/or exception types, or incorrect
exception types.

%leverages XRank to detect the exception-related bugs, which are the
%StackOverflow code snippets that were supposed to handle exceptions,
%but missed \code{try-catch} blocks and/or exceptions.

%We used {\tool} to detect such bugs and compare with FuzzyCatch.

{\em Procedure.} We trained {\tool} on the GitHub dataset and detected
the exception-related bugs~\cite{fuzzycatchbugs} in FuzzyCatch dataset of
incomplete code snippets. If exception handling in a snippet
matches exactly with {\tool}'s suggestion, we consider it as~correct.

%To make the FuzzyCatch dataset a balanced dataset for a fair comparison, we randomly select the same number of non-buggy code snippets comparing the bugs as the negative samples from the methods in the FuzzyCatch dataset.

%{\em Metrics.}
%We use {\bf Recall}, {\bf Precision}, and {\bf F1-score} as in RQ1.

\vspace{2pt}
\subsubsection{Ablation Study (RQ6)}~\\
We aim to evaluate the contribution of fine-tuning in {\tool}. We
compared {\tool} against CodeBERT without fine-tuning.
