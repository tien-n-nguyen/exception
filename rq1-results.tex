\subsection{Comparison on Try-Catch Necessity Checking Effectiveness (RQ1)}
%\label{sec:rq1}

\begin{table}[t]%[htpb]
  \caption{Try-Catch Necessity Checking Comparison (RQ1)}
  \vspace{-12pt}
  \small
	\begin{center}
		\renewcommand{\arraystretch}{1}
		\begin{tabular}{|p{1.75cm}<{\centering}|p{1.80cm}<{\centering}|p{1.80cm}<{\centering}|}
		  \hline
			\multirow{2}{*}{} & \multicolumn{2}{c|}{Github Dataset} \\
			\cline{2-3}
			  & \tool  & CodeBERT (w/o fine-tuning) \\
			\hline
			Recall    & 0.9842 & \textbf{0.9944}\\
			Precision & \textbf{0.9805} & 0.5010\\
			F-score   & \textbf{0.9824} & 0.6663\\
			\hline
		\end{tabular}
		\label{tab:xblock}
	\end{center}
\end{table}

%{\color{red}{This section waiting for the XRank Results. But from the current estimate, our approach should have higher F-score. But the recall and precision I'm not sure. Once I have the results, I will update this section.}}

%Table~\ref{tab:xblock} displays the comparison result.

As seen in Table~\ref{tab:xblock}, {\tool} achieves high performance
across two datasets. With a Precision of 68\%, it can decide correctly
2 out of 3 cases if a code snippet needs a \code{try-catch}
block or not. With a Recall of 79\%, {\tool} covers 4 out of
5 cases that needs to be placed in a \code{try-catch} block. Users
just need to find 1 out of 5 cases. As a result, it achieves a high
F-score of 0.73.
%In FuzzyCatch dataset, {\tool} also achieves a high level of
%performance with XX\% precision, YY\% recall, and ZZ\% F-score.
In comparison, {\tool} improves relatively over XRank 28.3\% in
Precision and 12.3\% in F-score.

%the state-of-the-art approach, XRank, {\bf -7.1\%} in Recall, {\bf
%  28.3\%} in Precision, and {\bf 12.3\%} in F-score.

%In FuzzyCatch dataset, the relative improvements are XX\%, YY\%, and
%ZZ\% in precision, recall, and F-score, respectively.

%We examined closely the cases that {\tool} performed better than
%XRank.

Examining the result, we reported the following. First, if the
association score of {\em only one API method} in the snippet and {\em one
exception} is higher than a threshold, XRank decides that a
\code{try-catch} block is needed.  Thus, it often tends to output
  ``Yes''. {\em Its recall is slightly
  better, but precision is just marginally better than a coin toss
  (0.53) in our balanced dataset. That leads to lower F-score than {\tool}}.
%
Second,
%XRank relies on the association scores between the presence of
%API method calls and the presence of a \code{try-catch} block.
the decisions on the necessity of a \code{try-catch} block or the
exception types depend on the pre-defined thresholds in XRank on those
association scores. Thus, those pre-defined thresholds might not be
suitable across the libraries. Third, for the incomplete code
snippets in which the names of the API methods in different packages
or libraries are the same (e.g., \code{toString} or \code{getText} in
various JDK packages), XRank cannot distinguish them and use one entry
in the dictionary for them due to its IR approach. In contrast, unlike
XRank which considers only the API method calls in a \code{try-catch}
block, {\tool} considers the code in the block as the context to learn
the program dependencies/relations among the names of those API
elements. That is, it leverages the relations among the names of API
elements to learn their identities, thus, deciding better the need of
\code{try-catch} blocks and the corresponding exception types.

%Tien:RQ2 Table
\begin{table}[t]
  \caption{Try-Catch Statement Detection Effectiveness (RQ2)}
  \vspace{-12pt}
	\begin{center}
		\small
		\renewcommand{\arraystretch}{1} 
		\begin{tabular}{p{0.8cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}|p{0.4cm}<{\centering}}
			\hline
			 	&  \multicolumn{10}{c}{Accuracy} \\
			\cline{2-11}
			     	&  N1  & N2   &  N3  & N4   &N5    & N6   &N7    & N8   &N9    & N10 \\
			\hline
			\tool     & 0.42 & 0.47 & 0.59 & 0.66 & 0.71 & 0.74 & 0.76 & 0.77 & 0.78  & 0.79  \\
			\hline
		\end{tabular}
		Nx is number of nodes in the explanation
                sub-graph (\code{try-catch} block)
		\label{tab:rq2}
	\end{center}
\end{table}


Take as an example a code snippet (not shown) in our dataset with the
presence of \code{getText}. This name is popular with a very large number
of API method candidates.
%For example, in a code snippet, \code{getText} has a very large number
%of API method candidates.
However, considering the relation between \code{css} and
\code{getText} in the code \code{`...css()\-.getText()'},~the number
of candidates for \code{getText} is only 4. Finally, considering~the
return value of \code{getText} as an argument of
\code{setInnerText(...)} in the code
\code{`setInnerText(...css()\-.getText())'}, only one candidate is
remained:
\code{com\-.google\-.gwt\-.resources\-.client\-.CssResource\-.getText()}.
Thus, those relations actually help identify the API elements,
leading to better decision in {\tool} on the \code{try-catch} block
and exception types.
%
Because it has not seen any \code{try-catch} block involving
\code{com\-.....getText()} and those related ones, {\tool} decides
that the code snippet does not need a \code{try-catch} block. In
contrast, XRank considers only the {\em pairwise} associate scores
between an {\em individual API method call} and the exception types
in a \code{catch} clause. It disregards those above
relations/dependencies among the API names. Thus, it might
misunderstand that \code{getText} needs a \code{try-catch} due to the
co-occurrences of other API elements that need one. That is, without
the dependencies, XRank might make incorrect identification of the API
elements via their names, leading to incorrect exception
recommendation.

%considering Groum but only to get better API ...


%\begin{table}[htpb]
%  \caption{Try-Catch Necessity Checking Comparison (RQ1)}
%  \vspace{-12pt}
%	\begin{center}
%		\renewcommand{\arraystretch}{1}
%		\begin{tabular}{p{1.5cm}<{\centering}|p{1.25cm}<{\centering}p{1.25cm}<{\centering}|p{1.25cm}<{\centering}p{1.25cm}<{\centering}}
%			\hline
%			\multirow{2}{*}{} & \multicolumn{2}{c|}{{\tool} Dataset} & \multicolumn{2}{c}{FuzzyCatch Dataset}\\
%			\cline{2-5}
%			  & \tool  & XRank & \tool  & XRank\\
%			\hline
%			Recall    & \textbf{0.81} & &&\\
%			Precision & \textbf{0.66} & &&\\
%			F-score   & \textbf{0.73} & &&\\
%			\hline
%		\end{tabular}
%		\label{tab:xblock}
%	\end{center}
%\end{table}
