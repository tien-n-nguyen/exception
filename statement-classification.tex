\section{{\xstate}: \code{Try-block} Statement Detector}
\label{sec:xstate}


Figure~\ref{fig:xstate} illustrates the architecture of {\xstate}, the
\code{try}-block statement detection component.
%goal and model
The goal of {\xstate} is to decide if a
statement in the given code snippet needs to be placed within~a~\code{try}
block. For code representation learning, we use one single
CodeBERT~\cite{codebert-emnlp20} model as in {\xblock} (see
Figure~\ref{fig:xblock}) to produce the embedding for the code
(sub)-tokens. For the input code, a \texttt{[SEP]} token represents
the statement preceding~it.
%Note that semicolon would not be as consistent as our explicit
%separator, because some statements might not end with a semicolon. A
%semicolon also could appear inside string literals and for-loop
%conditions.

%rationale on why the model works
The advantage in using CodeBERT on the entire code snippet has two
folds. First, as in Key Idea 2, the context of the entire code
facilitates {\tool} to learn the {\em relations of an API and its
  exception types}, thus, making the connection between the API and
the presence of \code{try-catch} block. For example, in
Figure~\ref{fig:xstate}, \code{write} in the statement $s1$ could be
determined as having a relation with \code{IOException},
leading to better learning to place that statement inside a
\code{try} block. Second, we expect that CodeBERT would learn
     {\em the dependencies among the consecutive statements} separated
     by the special tokens \texttt{[SEP]} (see Section~\ref{sec:rq4} for our
     experiment on this). Such dependencies would help the model better
     decide whether some consecutive statements need to be placed
     together in a \code{try} block. For example, in
     Figure~\ref{overview}, the statements at lines 2--5 have data
     dependencies, thus, they might be in the same \code{try}
     block.

%encoding
We take the embedding produced by CodeBERT for each \texttt{[SEP]} token
as the statement embedding and feed it to a layer with Feed-forward
neural network (FFNN). We then use a softmax function to learn to
label each corresponding statement with one of three~tags: {\em
  \code{O} tag means that the statement is outside of any
  \code{try} blocks; \code{B-Try} tag means that the statement
  begins a \code{try} block; and \code{I-Try} tag means that the
  statement is inside a \code{try} block and is not the first
  line of that block}. For example, in Figure~\ref{fig:xstate}, the
embedding computed by CodeBERT for the first \texttt{[SEP]}, which
represents the statement \code{out.write(...)}, is classified by the
first \code{FFNN+Softmax} layer into the \code{B-Try} category/label
because it is the first statement of a \code{try}
block. However, the embedding for the second \texttt{[SEP]}, representing
\code{print(node.getBody(...))}, is labeled as \code{I-Try} because it
is the second statement of the \code{try} block. Finally, the
embedding of the third \texttt{[SEP]}, is labeled as \code{O} because the
statement \code{return null;} does not need to be in the
\code{try} block. With this IOB2 encoding~\cite{iob2}, we can
support multiple \code{try-catch} blocks in which the statement with
\code{B-Try} is the start of a \code{try} block and the
statement with the last respective \code{I-Try} tag is the end of that
block. A new block is formed as another statement having a
\code{B-Try} tag.

\begin{figure}[t]
 	\centering
 	\includegraphics[width=3.4in]{xstate-4.png} %xstate-3: 3.2
        \vspace{-18pt}
 	\caption{\code{Try-catch} Statement Detector ({\xstate})}
 	\label{fig:xstate}	
\end{figure}


%training/predicting
In training, the labels for all statements are known from the
code. In  prediction, {\tool} will assign IOB2 labels to the
statements.

%, from which we can derive the \code{try-catch} blocks and
%what statements belonging to each block.


%of all the lines are known from the code. During prediction, the model will be able to assign tags to all the lines in the code snippet. From these tags, we get to know how many try-blocks should be added, and which lines of code belong to which try-block.

%{\xstate} also uses CodeBERT, the same one as in {\xblock}. However, we create a separate figure for each module so that it is easier to follow. 

%Each \texttt{[SEP]} token represents the line of code preceding it (Note that semicolon would not be as consistent a line separator as the \texttt{[SEP]} token, since a line of code may not end with a semicolon. And a semicolon often appears inside string literals and inside for-loop conditions. ). We expect that the relationship between lines of code can be learnt through these \texttt{[SEP]} tokens during training, so that at prediction time, the model can determine for each line as to whether it is inside or outside a try-catch block. We devise three tags in the IOB2 format to accomplish this task: \texttt{O} tag means that the statement is outside any try-blocks; \texttt{B-Try} tag means the statement begins a try-block; and \texttt{I-Try} tag means the statement is inside a try-block and is not the first line in that try-block. 





% This section presents {\xstate} and how we leverage an Explainable AI
% model, GNNExplainer~\cite{GNNExplainer}, to detect what statements in
% the given code snippet $C$ need to be placed in a \code{try-catch} block
% provided that {\xblock} had decided the necessity of such a block.

% %Tien
% {\xstate} takes the following as inputs: 1) the PDG ($G_C$) of the
% given code $C$, 2) the trained R-GCN model for {\xblock}, along with
% the positive detection result $\mathcal{V_C}$ and the prediction score.
% Figure~\ref{fig:GNNEX} displays an illustration of the process in
% {\xstate}.

% % Let us explain how we use GNNExplainer~\cite{GNNExplainer} to build
% %our graph-based interpretation. The input includes the trained FA-GCN
% %model, the PDG ($G_M$) of the method $M$, and the detection result
% %$\mathcal{V}$ or $\mathcal{NV}$, and prediction score.

% %Figure~\ref{fig:GNNEX} illustrates our process for the case of
% %$\mathcal{V}$ (Vulnerable) (the case of $\mathcal{NV}$ is done
% %similarly).



% %The interpretations include 1) a crucial sub-graph $\mathcal{G}_M$,
% %corresponding to the PDG sub-graph consisting of statements relevant
% %to the vulnerability, and 2) a subset of crucial features
% %$\mathcal{X}_M$, corresponding to the set of variables involving to
% %the vulnerability.

% To decide the statements in the given code to be placed in a
% \code{try-catch} block, we aim to determine the statements that are
% most decisive and crucial for the {\xblock} model in deciding its
% outcome of ``Yes'' (i.e, {\xblock} decides that the snippet must be
% placed in a \code{try-catch} block). Therefore, we formulate that as
% the following problem: finding a sub-graph $\mathcal{G}_C$ in the PDG
% $G_C$ for $C$ that minimizes the difference in the prediction scores
% between using the entire graph $G_C$ and using the minimal graph
% $\mathcal{G}_C$. To do so, we leverage the Explainable AI model,
% GNNExplainer~\cite{GNNExplainer}. It uses a masking technique in which
% instead of searching for that minimal subgraph, it learns the {\em
%   edge-mask} set $EM$ of the edges and the {\em feature-mask} set $FM$
% of the features. Learning those two sets $EM$ and $FM$ helps derive
% the explanation sub-graph $\mathcal{G}_C$ and the set of crucial
% feature $\mathcal{X}_C$ by masking-out the edges in $EM$ from $G_C$,
% and the feature in $FM$ from $X_C$:
% \begin{equation}\label{eq:11}
% \mathcal{G}_C = G_C \bigodot EM
% \end{equation}
% \begin{equation}\label{eq:12}
% \mathcal{X}_M = X_M \bigodot FM
% \end{equation}
% $\bigodot$ is used to denote the ``masking-out'' operation.

% %To derive the interpretations, the key goal is to find a sub-graph
% %$\mathcal{G}_M$ in the PDG $G_M$ of the method $M$ that minimizes the
% %difference in the prediction scores between using the entire graph
% %$G_M$ and using the minimal graph $\mathcal{G}_M$. To do so, we use
% %GNNExplainer with the {\em masking technique}~\cite{GNNExplainer},
% %which treats the searching for the minimal graph $\mathcal{G}_M$ as a
% %learning problem of the {\em edge-mask} set $EM$ of the edges.  The
% %idea is that learning $EM$ helps {\tool} derive the interpretation
% %sub-graph $\mathcal{G}_M$ by masking-out the edges in $EM$ from $G_M$
% %(``masked-out'' is denoted by
% %$\bigodot$): \begin{equation}\label{eq:11} \mathcal{G}_M = G_M
% %\bigodot EM \end{equation}

% Figure~\ref{fig:GNNEX} illustrates the principle. Note that the
% trained R-GCN model in {\xblock} had decided that a \code{try-catch}
% block is needed for the given code (in which the entire original PDG
% was used as input). When an edge-mask set is applied, some of the
% edges (and the inducing nodes) are masked. With the new graph being
% used as~the new input, if the trained R-GCN model in {\xblock}
% produces ``Yes'' as the result, i.e., {\em the classification does not
% change, then the edges in the edge-mask set are not important}. Thus,
% they will not be included in the explanation graph $\mathcal{G}_C$. If
% the trained R-GCN model in {\xblock} with the new input graph produces
% ``No'' as the result, i.e., {\em the classification does change from
% ``Yes'' to ``No'', then those edges in the edge-mask set are important
% to the model, thus, being included in the explanation graph
% $\mathcal{G}_C$}. Similar logic is applied to include or exclude the
% features in the {\em feature-mask} $FM$.



% %Tien Figure~\ref{fig:GNNEX} illustrates GNNExplainer's principle. As
% %an edge-mask set is applied, GNNEXplainer checks if the FA-GCN model
% %produces the same result~(in this case the result is
% %$\mathcal{V}$). If yes, the edge in the edge-mask is not important
% %and is not included in $\mathcal{G}_M$. Otherwise, the edge is
% %important and included in $\mathcal{G}_M$.  Because the numbers of
% %possible sub-graphs and the edge-mask sets are untractable,
% %GNNExplainer uses a learning approach for the edge-mask $EM$.

% %\begin{equation}\label{eq:12}
% %\mathcal{X}_M = X_M \bigodot FM
% %\end{equation}

% %For example, as you can see in figure \ref{fig:GNNEX}, when the
% %GNNExplainer get the whole PGD $G_m$, it starts to use the
% %\textit{edge-mask} $EM$ and \textit{feature-mask} $FM$ to mask some
% %edges and features.  As you can see in the second row of the figure,
% %after masked some edges from $G_m$ to generate a graph $G'_m$,
% %GNNExplainer use the detection model to check the new graph to see
% %how the detection result changes. If the result has been influenced a
% %lot, it means that the masked edges are important for the detection
% %result. If not, it means that the masked edges are not important for
% %the detection result. It is similar for the \textit{feature-mask}
% %$FM$.  Because in the last step, we could get $F_{v}$ for each
% %statement $v$ and $F_{v}$ have the same dimension, $FM$ masks the
% %features in the same position in $F_{v}$ for each statement $v$ and
% %then use detection model to evaluate the importance of the masked
% %features just as the edges. As you can see in figure \ref{fig:GNNEX},
% %$FM$ masked the second and the 4th feature for each statement in this
% %example.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=3in]{XAI.png}
%         \vspace{-0.06in}
% 	\caption{Masking to derive the explanation sub-graph containing statements in a \code{Try-Catch} block ({\xstate})}
%         \vspace{-0.06in}
% 	\label{fig:GNNEX}	
% \end{figure}

% %\begin{align}\label{maineq}
% %\nonumber
% %\max_{\mathcal{G}_M} MI(Y,(\mathcal{G}_M,\mathcal{X}_M)) = H(Y) - H(Y|G=\mathcal{G}_M,X=\mathcal{X}_M)
% %\end{align}

% %to minimizing conditional entropy
% %$H(Y|G=\mathcal{G}_M,X=\mathcal{X}_M)$

% %\begin{equation}
% %  \label{eq2}
% %-\EX_{Y|\mathcal{G}_M,
% %  \mathcal{X}_M} [log P_{FA-GCN} (Y|G=\mathcal{G}_M,X=\mathcal{X}_M)]
% %  \end{equation}

% %\begin{equation}
% %  \label{eq3}
% %  \min_{\mathcal{G}} \EX_{\mathcal{G}_M \sim \mathcal{G}} H(Y|G=\mathcal{G}_M,X=\mathcal{X}_M)
% %\end{equation}
% %\begin{equation}
% %  \label{eq4}
% %  \min_{\mathcal{G}} H(Y| G=\EX_{\mathcal{G}}[\mathcal{G}_M], X=\mathcal{X}_M)
% %\end{equation}
% %\nonumber

% Because the numbers of possible sub-graphs, edge-mask sets, and
% feature-mask sets are untractable, GNNExplainer uses a learning
% approach for $EM$ and $FM$. It aims to maximize the mutual information
% (MI) between the minimal graph $\mathcal{G}_C$ and the input
% PDG~$G_C$~\cite{GNNExplainer}:
% \begin{equation}\label{maineq}
% \max_{\mathcal{G}_C} MI(Y,\mathcal{G}_C) = H(Y) - H(Y|G=\mathcal{G}_C)
% \end{equation}
% $Y$ is the outcome decision by the trained R-GCN model. Thus, the
% entropy term $H(Y)$ is constant for the trained R-GCN
% model.

% Maximizing the $MI$ value for all $\mathcal{G}_C$s is equivalent
% to minimizing conditional entropy $H(Y|G=\mathcal{G}_C)$, which by
% definition of conditional entropy can be expressed~as
% \begin{equation}
%   \label{eq2}
% -\EX_{Y|\mathcal{G}_C}
%   [log P_{R-GCN} (Y|G=\mathcal{G}_C)]
% \end{equation}
% This conditional entropy is a measure of how
% much uncertainty remains about the outcome $Y$ when we know
% $G=\mathcal{G}_C$.
% %
% %GNNEXplainer also limits the size of $\mathcal{G}_C$ by $K_C$, {\em
% %  i.e.}, taking $K_C$ edges that give the highest mutual information
% %with the prediction outcome $Y$.
% %
% %Direct optimization of the formula(~\ref{eq2}) is not tractable, thus,
% %GNNExplainer treats $\mathcal{G}_C$ as a random graph variable
% %$\mathcal{G}$. The objective in Equation(~\ref{eq2}) becomes:
% %\begin{equation}
% %  \label{eq3}
% %  \min_{\mathcal{G}} \EX_{\mathcal{G}_C \sim \mathcal{G}} H(Y|G=\mathcal{G}_C)
% %\end{equation}
% %\begin{equation}
% %  \label{eq4}
% %  \min_{\mathcal{G}} H(Y| G=\EX_{\mathcal{G}}[\mathcal{G}_C])
% %\end{equation}
% %From Equation~\ref{eq3}, we obtain Equation~\ref{eq4} with Jensen's
% %inequality.  The conditional entropy in Equation~\ref{eq4} can be
% %optimized by replacing $\EX_{\mathcal{G}}[\mathcal{G}_C]$ to be
% %optimized by masking with $EM$ on the input graph $G_C$.  Now, we can
% %reduce the problem to learning the mask $EM$.  Similar logic is
% %applied to $FM$.

% More details can be found in~\cite{GNNExplainer}. The resulting
% sub-graph $\mathcal{G}_C$ is used to derive the statements to be
% placed in a \code{try-catch} block. Note that, we can set the
% statements to be consecutive.
