\section{Multi-task Learning}
\label{sec:multitasking}

%Learning on the three tasks, {\xblock}, {\xstate}, and {\xtype}
%can benefit to one another. If a model decides the need
%of a \code{try-catch} block, there must be some statements in the code
%snippet that will be placed in such a block. If a model learns the
%statements to be placed in a \code{try} block, it can decide
%that the code snippet needs such a block and make the connections to
%what exception types to be caught. The knowledge on the exception
%types can help a model decide better what important statements need to
%be in a \code{try-catch} block. Thus, we put the three tasks
%in a multi-task learning fashion.

Mutual enhancement can occur through learning on the three tasks:
{\xblock}, {\xstate}, and {\xtype}. When a model discerns the
necessity of a \code{try-catch} block, it implies the existence of
specific statements within the code snippet that warrant such a
block. By training a model to recognize the statements to be placed in
a \code{try} block,~it~gains the ability to determine the need for a
\code{try-catch} block and establish associations with the respective
exception types. Moreover, the model's knowledge of exception types
aids in making informed decisions about which crucial statements
should be in a \code{try} block. Thus, we
put these three tasks in a multi-task learning scheme.


%The two tasks dealt with by {\xstate} and {\xtype} can benefit each other. With the learning about which statements need to be put into try blocks, the model will learn the important code tokens in the code snippet, which in turn will help to determine which exceptions need to be handled. Also, knowing the exceptions will give strong hint to the way that statements should be grouped into try blocks.

We calculate the training loss by combining the losses from the three
tasks. $Loss_{{\xblock}}$ is the Binary Cross Entropy loss for the
decision as to if \code{try-catch} block(s) is needed. To calculate
$Loss_{{\xstate}}$, we add the classification losses for all
statements in the input, where a statement loss ($Loss_{Stmt}$) is the
Cross Entropy loss calculated from the distribution of the three tags
(\code{O}, \code{B-Try}, \code{I-Try}) and the ground-truth tag.
Finally, in \xtype, since several \code{try-catch} blocks might be
present, $Loss_{{\xtype}}$ comes from the summation of the exception
prediction losses from all the \code{try-catch} blocks. For each
\code{try-catch} block, $loss_{Try-block}$ is calculated by adding
the Binary Cross Entropy loss for the prediction of each exception of
interest.

The overall training loss is calculated as follows. If the input does
not contain any \code{try-catch} block, the overall loss will be the 
$Loss_{{\xblock}}$. If the input contains a \code{try-catch} block,
the overall loss will be the summation of losses from all
three tasks:
\begin{equation}
\label{eqn:loss}
Loss_{Overall} =
\begin{cases}
Loss_{{\xblock}},  & \text{no \code{try-catch}}\\
Loss_{{\xblock}} + Loss_{{\xstate}} + Loss_{{\xtype}}, &\text{otherwise.}
\end{cases}
\end{equation}
