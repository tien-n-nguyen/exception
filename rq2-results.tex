\subsection{Try-Catch Statement Detection (RQ2)}
\label{sec:rq2}

\begin{table}[t]%[htpb]
  \caption{Try-Catch Statement Detecting Comparison ({\xblock}+{\xstate}, Instance Level) (RQ2)}
%  \caption{Try-Catch Statement Detecting Comparison (Evaluate {\xstate} in Connection with {\xblock} --- Instance Level) (RQ2)}
  \vspace{-12pt}
  \small
	\begin{center}
		\renewcommand{\arraystretch}{1}
		\begin{tabular}{| p{3.05cm}<{\centering} | p{1.2cm}<{\centering} | p{1.2cm}<{\centering}| p{1.2cm}<{\centering}|}
		  \hline
		Instance Level	  & Precision  & Recall & F1-score \\
			\hline
                        %			CodeBERT w/o fine-tuning &  0.0 & 0.0  & 0.0\\
                        GPT-3.5 & 0.550 & 0.232 & 0.326 \\
			\hline
			\xblock + \xstate   & \textbf{0.969}  &  \textbf{0.607} & \textbf{0.747}\\
			\hline
		\end{tabular}
		\label{tab:xstate-1}
	\end{center}
\end{table}

\begin{table}[t]%[htpb]
\caption{Try-Catch Statement Detecting Result ({\xstate} as Individual, Instance Level) (RQ2)}
  \vspace{-12pt}
  \small
	\begin{center}
		\renewcommand{\arraystretch}{1}
		\begin{tabular}{| p{3.05cm}<{\centering} | p{1.2cm}<{\centering} | p{1.2cm}<{\centering}| p{1.2cm}<{\centering}|}
		  \hline
		Instance Level	  & Precision  & Recall & F1-score \\
			\hline
%			GPT-3.5 &   &   & \\
			\hline
			\xstate  & \textbf{1.0}  &  \textbf{0.6198} & \textbf{0.7652}\\
			\hline
		\end{tabular}
		\label{tab:xstate-2}
	\end{center}
\end{table}

Table~\ref{tab:xstate-1} shows the result when we evaluated {\xstate}
in connection with {\xblock}. That is, both individual results from
{\xblock} and {\xstate} must be correct for the instance to be
considered correct. {\tool} achieves a very high precision (96.88\%)
and predicts correctly all the statements in \code{try-catch} blocks
for about 60\% of the positive code instances. As seen, {\tool}
improves relatively over GPT-3.5 {\bf 76.1\%}, {\bf 161.7\%}, and {\bf
  128.7\%} in Precision, Recall, and F1-score. Examining the results,
we found that GPT-3.5 did not work well for the code snippets that
have more than one \code{try-catch} blocks. It also does not recognize
well multiple statements with dependencies that need to be placed in
the same block. {\tool} recognizes well the dependencies among
statements (see RQ4), thus, better grouping them into a
\code{try-catch} block.

%several statements in which some statements are left outside of the
%blocks.

Table~\ref{tab:xstate-2} displays the result when we evaluated
{\xstate} individually (i.e., assuming {\xblock} correctly predicts
the presence of \code{try-catch} blocks). As seen, the numbers are
slightly higher than those for {\xblock}+{\xstate} because there is no
impact from {\xblock}'s result. In other words, {\xstate} manages to
achieve a 100\% precision, showing that {\xstate} is capable of giving
correct predictions for those false negatives from {\xblock}.

%As Table~\ref{tab:xstate-1} shows, when evaluating {\xstate} in
%connection with {\xblock}, {\xstate} achieves high precision score and
%is able to recover statements in try-catch blocks for about 60\% of
%the positive code samples, while the codebert baseline model fails the
%task completely.  Furthermore, we present results of evaluating
%{\xstate} as individual in Table~\ref{tab:xstate-2}. As can be seen,
%the codebert baseline still cannot give any correct predictions. In
%comparison, {\xstate} manages to achieve a 100\% precision, showing
%that {\xstate} is capable of giving correct predictions for those
%false negatives from {\xblock}.

%\input{rq2-block}
%------------------------

\begin{table}[t]%[htpb]
  \caption{Try-Catch Statement Detecting Comparison ({\xblock}+{\xstate}, Statement Level) (RQ2)}
  \vspace{-12pt}
  \small
	\begin{center}
		\renewcommand{\arraystretch}{1}
		\begin{tabular}{| p{3.05cm}<{\centering} | p{1.2cm}<{\centering} | p{1.2cm}<{\centering}| p{1.2cm}<{\centering}|}
		  \hline
		Statement Level	  & Precision  & Recall & F1-score \\
			\hline
                        GPT-3.5 & {\bf 0.XX} & {\bf 0.YY} & {\bf 0.ZZ} \\
			\hline
			\xblock + \xstate   & \textbf{0.XX}  &  \textbf{0.YY} & \textbf{0.ZZ}\\
			\hline
		\end{tabular}
		\label{tab:xstate-3}
	\end{center}
\end{table}

\begin{table}[t]%[htpb]
\caption{Try-Catch Statement Detecting Result ({\xstate} as Individual, Statement Level) (RQ2)}
  \vspace{-12pt}
  \small
	\begin{center}
		\renewcommand{\arraystretch}{1}
		\begin{tabular}{| p{3.05cm}<{\centering} | p{1.2cm}<{\centering} | p{1.2cm}<{\centering}| p{1.2cm}<{\centering}|}
		  \hline
		Statement Level	  & Precision  & Recall & F1-score \\
			\hline
%			GPT-3.5 &   &   & \\
			\hline
			\xstate  & \textbf{0.XX}  &  \textbf{0.YY} & \textbf{0.ZZ}\\
			\hline
		\end{tabular}
		\label{tab:xstate-4}
	\end{center}
\end{table}

Table~\ref{tab:xstate-3} displays the result at the statement level
(i.e., whether a statement needs to be inside a \code{try-catch} block
or not). As seen, {\xblock}+{\xstate} improves relatively over GPT-3.5
{\bf XX.X\%}, {\bf YY.Y\%}, and {\bf ZZ.Z\%} in Precision, Recall, and
F1-score, respectively in prediction at the statement level. As seen
in Table~\ref{tab:xstate-4}, {\tool} also achieves high numbers in
prediction at the statement level as individual.

%--------------------------
%Table~\ref{tab:rq2} displays the result on detecting the statements
%that need to be placed in a \code{try-catch} block. $N_x$ is a parameter
%in GNNExplainer that defines the number of nodes in the explanation
%graph $\mathcal{G}_C$, i.e., the number of statements to be placed in the
%\code{try-catch} block.
%%
%As the number of nodes (statements) in $\mathcal{G}_C$ increases, the
%number of correct statements covered also increases, thus, accuracy
%increases. However, as the number of statements increases higher than
%5, accuracy increases more slowly. In our dataset, the average size of
%a \code{try-catch} block is 5.9 statements. As seen, the accuracy as
%$N_x$=6 is 74\%. That is, by pointing out 6 statements on average,
%{\tool} can correctly suggest 74\% of the total number of statements
%in the dataset that need to be placed in \code{try-catch}
%blocks. That is, it points out correctly 4.5 out of 6 statements to be
%in a \code{try-catch} block. For the statements that do not need to be
%placed in a \code{try-catch} block, {\tool} predicts correctly with
%63\% accuracy (not shown).
%
%\input{example-experiment}
%
%\vspace{2pt}
%\noindent {\bf Example.} Figure~\ref{fig:example-experiment} displays
%an example that {\tool} made correct suggestions. {\bf First}, it made
%a correct suggestion on the need of a \code{try-catch} block for the
%code at lines 1--8, 16.  {\bf Second}, GNNExplainer pointed out that
%{\xblock} used all the statements at lines 3--8 for such correct
%prediction. As a consequence, {\tool} correctly suggests to place
%lines 3--8 into a \code{try-catch} block. Note that, it also correctly
%pointed out that lines 1 and 16 do not need to be inside the
%\code{try-catch} block.  {\bf Third}, GNNExplainer gives three
%statements at lines 3, 6, and 8 highest scores. We can see that those
%lines contain three crucial API method calls: 1)
%\code{FileInputStream}, 2) \code{read}, and 3) \code{close}. {\bf
%  Fourth}, those three lines have data and control dependencies, which
%could help the model learn the identities of the API elements via
%their names \code{FileInputStream}, \code{read}, and \code{close},
%despite that the code snippet does not have the fully-qualified names
%for those API elements. This confirms the need of integrating {\em program
%  dependencies} in our solution. {\bf Finally}, {\tool} was also able
%to learn from the training corpus that those names refer to those API
%elements, which often correspond to the following exception types: 1)
%\code{FileInputStream} with \code{FileNotFoundException}, and 2)
%\code{FileInputStream.read} and \code{FileInputStream.close} with
%\code{IOException}.


%Note that {\tool} via {\xstate} predicts the statements to be placed
%in the \code{try-catch} block only after {\xblock} predicted that the
%given code needs such a block. Therefore, the incorrect cases from
%{\xblock} (i.e., those cases that need to be in a \code{try-catch}
%block but were predicted not) are also counted as incorrect in
%{\xstate}.

%{\color{red}{N1-N10 are the number of nodes that the subgraph contains which means the size of the try-catch block. Because the average size of the try-catch is 5.7, I currently pick 6 as the size of the try-catch block. The accuracy here is defined as: if a statement is in the try-catch block and our model put it in the subgraph, I regard it is correct $S_c$. All other conditions, I think they are incorrect. If there are $S$ statements in the try-catch block, the total accuracy is calculated as $S_c/S$. Later, I will add an example showing that the statement that our model predicted in the try-catch block contains the method call which lead to the correct exception types prediction.}}
