\vspace{-6pt}
\subsubsection{{\xtype}'s Effectiveness on Exception Type Recommendation (RQ3)}

\indent {\em Baselines.} We compared {\xtype} against GPT-3.5 as in RQ1.

%the pre-trained CodeBERT model without any fine-tuning step as in RQ1.

%We compared {\tool} against the state-of-the-art exception type recommendation approach XRank~\cite{nguyen2020code}.

%{\em Procedure.} We used the same procedure as in RQ1.

%In this RQ, \tool is evaluated on the DeepEx dataset. Because only the data points that \tool can successfully predict all statements in the try-catch block successfully, the \tool will run the exception type prediction on them. Similarly, as RQ2, to fully use the dataset and evaluate \tool on more data, here we estimate the Accuracy of RQ2 prediction is 100\% which means that we use all positive data in DeepEx dataset as the data we use in this RQ. We do the same data split as RQ1 and RQ2 to train, validate and test the model performance.

%{\em Tuning.} We used the same tuning as in RQ1.

{\em Procedure and Metrics.} For a predicted set of exception types,
we used 1) Precision (defined as the ratio between the size of the
overlapping set between the predicted and oracle sets over the size of
the predicted one), 2) Recall (defined as the ratio between the size
of the overlapping set between the predicted and oracle sets and the
size of the oracle set), and 3) F1-score (harmonic mean of Precision
and Recall).

As with {\xstate}, we evaluated {\xtype} in two ways. First, we
evaluated {\xtype} in connection with {\xblock} and {\xstate}. We
consider a case as correct if all three components give correct
predictions.
%We also computed those metrics in the cases where {\xblock} and
%{\xstate} are correct at the instance level.
Second, we also evaluated {\xtype} as individual: we evaluated
{\xtype} with those metrics in the cases in which {\xblock} and
{\xstate} are correct at the instance level.

%{\xblock} gives a correct prediction on whether \code{try-catch}
%blocks are needed, {\xstate} produces correct tags for all the
%statements, and {\xtype} predicted correctly on the exceptions that
%need to be caught for all the \code{try-catch} blocks.  Second, we
%evaluate {\xtype} as individual: we assumed that both {\xblock} and
%{\xstate} have given 100\% correct predictions for their own tasks. To
%do so, we put the code snippets that need \code{try-catch} blocks with
%statements correctly labeled in the oracle and used {\xtype} to
%predict the exception types for all such blocks.
%========================================

%Let us use $E$ and $P$ to denote the set of the
%exception types in the oracle and the set of predicted
%ones for one code snippet in the testing set. We aim to measure 1) how
%precise {\tool} predicts the exception types ({\bf Precision}), i.e.,
%how accurate the predicted types in $P$, and 2) how much {\tool} can
%cover in its prediction with respect to the oracle ({\bf Recall}),
%i.e., how much of $E$ that the predicted set $P$ can cover. Toward
%measuring Precision and Recall, we define {\bf Hit-$n$} as the number
%of the cases in which the predicted set $P$ contains
%{\em at least} $n$ correct exception types, i.e., $P$ and $E$ overlaps
%{\em at least} $n$ exception types regardless of the sizes of both
%sets.
%
%In Github dataset, more than 98.1\% of the code snippets have 1--3
%exception types in a \code{catch} clause. Thus, we compute Hit-$n$,
%Precision, and Recall for the size of $E$ (|$E$|) from 1--3 and 3+,
%and for the size of $P$ (|$P$|) from 1--3 and 3+.
%%
%When computing Recall, we use the set $E$ as the basis. Recall at a
%size $N_E$ of $E$ is computed as the ratio between Hit-$n$ at that
%size and the total number of cases with that size $N_E$. We compute
%Recall for all $N_E$ = 1--3, and 3+, and $n$=1--$N_E$. We also define
%{\bf \code{Hit-All}$_{Rec}$} as \code{Hit}-$n$ when the number $n$ of
%overlapping exception types is equal to |$E$|, i.e., all exception
%types in the oracle set for a code snippet are covered.
%%
%When computing Precision, we use the set $P$ as the basis. Precision
%at a size $N_P$ of $P$ is computed as the ratio between Hit-$n$ at
%that size and the total number of cases with that size $N_P$. We
%compute Precision for all $N_P$ = 1--3, and $n$=1--$N_P$. We also
%define {\bf \code{Hit-All}$_{Prec}$} as \code{Hit}-$n$ when the number $n$
%of overlapping exception types is equal to |$P$|, i.e., all predicted
%types in the predicted set $P$ for a snippet are
%correct.


%In this RQ, we use \textbf{Hit-n} as the evaluation metrics. Hit-n
%here means within the true labeled statement set, there are at least
%\textbf{n} statements predicted correctly by \tool.
