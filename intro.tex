\section{Introduction}
\label{sec:intro}

The online question and answering (Q\&A) forums, e.g., StackOverflow
(S/O) provide important resources for developers to learn how to use
software libraries and frameworks. While the code snippets in an S/O
answer are good starting points, they are often incomplete with
several missing details, even with ambiguous references, etc.  Zhang
{\em et al.}~\cite{zhang-icse19} have conducted a large-scale
empirical study on the nature and extent of manual adaptations of the
S/O code snippets by developers into their GitHub repositories.  They
reported that the adaptations from S/O code examples to their GitHub
counterpart projects are prevalent. They qualitatively inspected all
the adaptation cases and classified them into 24 different adaptation
types. They highlighted several adaptation types including {\em type
  conversion}, {\em handling potential exceptions}, and {\em adding if
  checks}~\cite{zhang-icse19}. Among them, adding a \code{try-catch}
block to wrap the code snippet and listing the handled exceptions in
the \code{catch} clause are frequently performed,
%(132 cases out of 629 S/O examples),
yet not automated by existing tools.
%code integration techniques.

The adaptation process for exception handling is not trivial as Nguyen
{\em et al.}~\cite{xrank-fse20} have reported that it is challenging
for developers to learn and memorize what API methods could cause
exceptions and what exceptions need to be handled. Kechagia {\em et
  al.}~\cite{kechagia-msr14} found that 19\% of the crashes in Android
applications could have been caused by insufficient documented
exceptions in Android APIs. Thus, it is desirable to have an automated
tool to recommend proper exception handling for the adaptation of
online code snippets.

There exist several approaches to automatic recommendation of
exception
handling~\cite{barbosa-bsse12,chanchal-scam14,barbosa-tse18,barbosa-tse16,xrank-fse20,throw-ase22}. They
can be classified into four categories. The first category of
approaches relies on a few {\em program analysis heuristics} on
exception types, API calls, and variable types to recommend exception
handling code~\cite{barbosa-bsse12}. These heuristic-based approaches
do not always work in all cases due to incomplete code. The second
category utilized {\em exception handling policies},
which are enforced in all
cases~\cite{barbosa-tse16,barbosa-saner18}. However, the policies need
to be pre-defined and encoded within the recommending tools.  This is
not an ideal solution considering the fast evolution of software
libraries. To enable more flexibility than policy enforcement, the
third category leverages {\em mining algorithms} that derive similar
exception handling for two similar code
fragments~\cite{chanchal-scam14}. While avoiding hard-coding of the
rules, these mining approaches suffer the issue of how much similar
for two fragments to be considered as having similar exception
handling. For the mining approaches, deterministically setting a
threshold for frequent occurrences is also challenging.
%In fact, two API method calls in two different contexts might require
%to handle the same exception. For example, two fragments of code
%using JDK containing an opening of the files for different purposes
%in two contexts might need to catch and handle the \code{IOException}
%due to the call to \code{java.io.nio.new\-Buffered\-Reader}.

To provide more flexibility in code matching, the fourth category
follows {\em information retrieval}
(IR)~\cite{xrank-fse20}. XRank~\cite{xrank-fse20} takes as input
source code and recommends a ranked list of API method calls in the
code that are potentially involved in the exceptions in a \code{catch-try}
block. XHand~\cite{xrank-fse20} recommends the exception handling
code in a \code{catch} block for a given code. Both use a fuzzy set
technique to compute the associations between the API calls (e.g.,
\code{new\-Buffered\-Reader}) and the exceptions (e.g.,
\code{IOException}).

While the IR-based approach achieves higher accuracy than the
others~\cite{xrank-fse20}, it has key limitations. First, it is not
trivial to {\bf pre-define a threshold} for feature matching for a
retrieval of an exception type or an API element. The effectiveness of
those IR techniques depends much on the correct value of such
pre-defined threshold. Second, the IR-based techniques rely on the
lexical values of the code tokens and API elements, whose names can be
{\em ambiguous} in an incomplete code snippet. For example, the
\code{Document} class in \code{org.\-w3c.\-dom} of the W3C library has
the same simple name as the \code{Document} class in
\code{com.\-google.\-gwt.\-dom.\-client.\-Document} of Google Web
Toolkit library (GWT). An API method to open/write/read a
\code{Document} in the W3C library might need to catch a different set
of exceptions than the one in GWT. Those IR-based techniques are not
sufficiently flexible to handle such {\bf ambiguous names}. Third, the
IR techniques {\em do not consider the {\bf context} of surrounding
  code}, thus, cannot leverage the {\em dependencies} among API
elements to resolve the ambiguity of the names of the APIs and
  exceptions in an incomplete snippet.

In this paper, we propose {\tool}, a learning-based exception handling
recommender, which accepts a given Java code snippet and recommends 1)
{\em whether a try-catch block is needed for the snippet} ({\xblock}),
2) {\em what statements need to be placed in a \code{try-catch} block}
({\xstate}) and 3) {\em what exception types need to be caught in the
  \code{catch} clause} ({\xtype}).  We find a motivation for such a
data-driven, learning-based approach from the previous studies
reporting that exception handling for the API elements is frequently
repeated across different
projects~\cite{chanchal-scam14,zhong-jss18}. The reason for such
repetitions is that the designers of a software library have the
intents for users to use certain API elements with corresponding
exception types. Thus, we design {\tool} to learn from the statements
in \code{try-catch} blocks and exception types retrieved from {\em
  complete source code} in a large code corpus, and derive suggestions
for given {\em (in)complete~code}.
%the above exception handling suggestions for the {\em (partial) code
%  snippet} under study.

We leverage and fine-tune the large language model
CodeBERT~\cite{codebert-emnlp20} to capture the surrounding code {\bf context}
with the dependencies among the API elements. Capturing such
contextual information and the dependencies enable {\tool} to realize
the idea {\em ``Tell Me Your Friends, I'll Tell You Who You Are''} to
learn the {\em {\bf dependencies} among statements with APIs} in a given
(in)complete code, leading to better learning in {\xblock}, {\xstate}
and {\xtype}. Inspired by sequence chunking in natural language
processing (NLP), we formulate our problem as detecting one or
multiple chunks of consecutive statements that need \code{try-catch}
blocks.
%Tien
{\tool} also has the three tasks in a {\bf multi-tasking} mechanism
to enable the mutual impact among the learning, leading to
better performance in all three tasks.

%The context also enables {\tool} learn to connect those statements
%with the corresponding exception types.
%that need to be caught in the \code{catch} clauses.

Our above idea gives {\tool} advantages over the state-of-the-art IR
approach. First,
%with the learning-based approach,
via learning, {\tool} does not rely on a pre-defined threshold for
explicit feature matching to retrieve API elements or
exceptions. Second, instead of using {\em the associations between
  pairs of an API and an exception type as in XRank},
%instead of learning only the associations between an API element and
%an exception type,
{\tool} relies on {\bf dependencies} and {\bf contexts}
%has advantages
in both predicting and training. During training, the complete code
provides the context for {\tool} to learn the dependencies among the
API elements and the exception types. During predicting, for a given
(in)complete code, {\tool} can implicitly match the current context
with such knowledge to avoid the name ambiguity and to derive the
exception handling suggestions.
%During predicting, for a given incomplete code, the surrounding code
%enables {\tool} to {\em learn the identities of the API elements via
%  the dependencies/relations} among them in the context, thus,
%avoiding the name ambiguity. Let us call it {\em dependency
%  context}. During training, the complete code enables the
%identifications (i.e., the fully-qualified names) of the API
%elements.
%
For example, encountering the program dependencies among the API
elements and exceptions in JDK, e.g., \code{get\-Declared\-Field} in
\code{java.\-lang.\-Class}, \code{get} in
\code{java.\-lang.\-Class.\-Field}, and
\code{No\-Such\-Field\-Exception} in the training data will help
     {\tool} learn to match the context in a given code to suggest
     \code{NoSuchFieldException}.

%Third, the context of surrounding code also helps the model implicitly
%learn the important features to connect between the API elements and
%the corresponding exception types.

%{\tool} is designed to capture the basic insights to overcome key
%limitations of the state-of-the-art approaches. First, with the
%learning-based approach, {\tool} does not rely on a pre-defined
%threshold for explicit feature matching for the retrieval of the API
%elements or exception types. Second, instead of learning only the
%associations between the API elements and corresponding exception
%types, we also consider the code context surrounding them. This gives
%{\tool} two advantages. During predicting, for a given incomplete
%code, the context enables {\tool} to {\em learn the identities of the
%  API elements via the dependencies/relations} among them in the
%context, thus, avoiding the name ambiguity. Let us call it {\em
%  dependency context}. During training, the complete code enables the
%identifications (i.e., the fully-qualified names) of the API
%elements. Moreover, the context of surrounding code also helps the
%model implicitly learn the important features to connect between the
%API elements and the corresponding exception types.

%We leverage Relational Graph Convolutional Network (R-GCN)~\cite{rgcn}
%to represent the program dependence graph (PDG) to capture the control
%and data dependencies among the API elements in the surrounding
%context. Learning the dependencies enables us to realize the key idea
%{\em ``Tell Me Your Friends, I'll Tell You Who You Are''} to learn the
%identities of the API elements in a given (in)complete code, leading
%to better learning in {\xblock}, {\xstate} and {\xtype}.
%
%Moreover, we treat the problem of predicting a \code{try-catch} block
%as a classification (Y/N) as well as the problem of deriving the
%handled exception types as a classification among exceptions. To
%predict which statements in a given code snippet that need to be
%wrapped in a \code{try-catch} block, we leverage the explainable ML
%model, GNNExplainer~\cite{GNNExplainer}, that {\em ``explains'' on
%why the GCN model has arrived at its decision}. Specifically, {\tool}
%takes two inputs: 1) the input PDG $G_C$ of the given code $C$, 2)
%the trained GCN model along with its decision on a \code{try-catch}
%block being needed for $C$. GNNExplainer will learn the explanation
%subgraph, which is defined as a minimal sub-graph $\mathcal{G}$ in
%the PDG of $C$ that {\em minimizes the prediction~scores between
%using the entire $G_C$ and using $\mathcal{G}$}. The idea is that if
%an~edge is {\em removed} from $G_C$, and {\em the decision of the
%model is affected, the edge is crucial and must be included in the
%explanation for the detection result}. Thus, the minimal sub-graph
%$\mathcal{G}$ in PDG contains the nodes and edges, {\em i.e.},
%including the {\em crucial statements} that are most
%decisive/relevant to the classification on exception blocks. We
%consider them as the statements to be placed in a
%\code{try-catch}~block.

%=================================================
We conducted several experiments to evaluate {\tool} with
a large dataset of 5,726 projects from GitHub
having 246,118 code snippets for training, 30,764 for validation,
and 30,764 for testing.
%30,764 code snippets (half of them have \code{try-catch} blocks).
Our empirical evaluation shows that {\tool} correctly performs all
three exception handling recommendation tasks in 71.5\% of the cases
with an F1-score of 70.2\%. It has an relative improvement of 166\%
over the baseline. It achieves high F1-score from 98.2\%--99.7\% in
\code{try-catch} block necessity checking (an relative improvement of
up to 55.9\% over the baselines). It also correctly decides both the
need of \code{try-catch} block(s) and the statements to be placed in
such blocks with the F1-scores of 74.7\% and 87.1\% at the instance
and statement levels, an improvement of 127.3\% and 44.9\% over the
baseline, respectively. Our extrinsic evaluation shows that {\tool}
relatively improves over the baseline by 56.5\% in F1-score in
exception-related bug detection in {\em incomplete} Android code
snippets.

%The result shows that {\tool} achieves a high accuracy of {\bf 98.3\%}
%and improves relatively by {\bf 56\%} over the state-of-the-art
%approach in \code{try-catch} block necessity checking. Moreover, it
%can correctly decide both the need of \code{try-catch} block(s) and
%the statements to be placed in such blocks in {\bf 79.4\%} of the
%cases, an improvement of {\bf 62X} over the baseline. Importantly,
%with an accuracy of {\bf 71.5\%} (an improvement of {\bf 146X} over
%the baseline), {\tool} correctly recommend exception handling in all
%three tasks. Our extrinsic evaluation also shows that with its
%recommendations, {\tool} improves by {\bf YY.Y\%} in F-score over the
%existing approach in detecting exception-related bugs.
%====================================================

%Our empirical evaluation shows that {\tool} relatively
%improves 12.3\% in F-score over the state-of-the-art approach,
%XRank~\cite{xrank-fse20} in \code{try-catch} block necessity checking.
%{\tool} also achieves a high accuracy of 74\% in recommending the
%statements to be placed in a \code{try-catch} block. It can cover all
%the needed exception types in 63\% of the cases and predict correctly
%all exception types in 33\% of the cases. Our extrinsic evaluation
%also shows that {\tool} improves 9.8\% in F-score over
%FuzzyCatch~\cite{xrank-fse20} in detecting exception-related~bugs.

%Our empirical evaluation shows that {\tool} achieves a high accuracy
%of XX\% in predicting if a code snippet needs a \code{try-catch}
%block. It also achieves a high accuracy of XX\% in recommending which
%statements in the snippet to be placed in a try-catch block. Our
%result shows that {\xtype} relatively improves over the
%state-of-the-art IR approach in exception type recommendation from
%XX\%â€“YY\% in accuracy.

In brief, this paper makes the following major contributions:

\noindent {\bf 1. [Neural Network-based Automated Exception Handling
    Recommendation]}. {\tool} is the first neural network approach to
automated exception handling recommendation in three above tasks.
{\tool} works for either complete or {\em incomplete} code.

\noindent {\bf 2. [Multi-tasking among three Exception Handling
    Recommendations]} We formulate the problem as sequence chunking
with a multi-tasking mechanism to learn for three above tasks.

%  \noindent {\bf 2. [Graph Neural Network and Explainable AI (XAI)]} We
%  leverage R-GCN to capture the program
%  dependencies among API elements and exceptions and leverage XAI
%  to derive the statements to be placed in a \code{try-catch} block.

  \noindent {\bf 3. [Empirical Evaluation]}. Our evaluation
  shows {\tool}'s high accuracy in exception handling recommendation
  as well as in exception-related bug detection. Data and code is
  available at~\cite{neurex-website}.


%for exception blocks, statements, and types.


